{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "https://droit-finances.commentcamarche.com/faq/15494-salaire-d-une-assistante-maternelle-ce-qu-il-faut-savoir\n",
      "[\"Salaire d'une assistante maternelle : ce qu'il faut savoir\"]\n",
      "[\"La rémunération des assistantes maternelles (salaire horaire, indemnités d'entretien et de nourriture...) doit respecter certains montants minimum. Voici un point sur les droits de l'assistante maternelle et les obligations des parents en...\"]\n",
      "================\n",
      "================\n",
      "https://droit-finances.commentcamarche.com/forum/affich-5732576-ne-pas-declarer-ses-revenus-a-pole-emploi\n",
      "['Ne pas déclarer ses revenus à pôle emploi [Résolu]']\n",
      "['Meilleure réponse: Bonsoir, La déclaration mensuelle est obligatoire et doit comportée impérativement tous les revenus que vous avez pu touché pour que justement Pôle Emploi puisse calculé vos droits. Pensez à ceci : Votre employeur va vous déclaré...']\n",
      "================\n",
      "================\n",
      "https://droit-finances.commentcamarche.com/faq/4843-aah-2019-montant-et-plafonds-de-ressources\n",
      "['AAH 2019 : montant et plafonds de ressources']\n",
      "[\"Le montant maximum de l'allocation aux adultes handicapés (AAH 2019) est de 860 euros depuis le 1er novembre 2018, date de sa dernière augmentation. Ce montant correspond à l'aide touchée par un bénéficiaire sans ressource. L'AAH est en...\"]\n",
      "================\n",
      "================\n",
      "https://droit-finances.commentcamarche.com/faq/7527-allocation-de-rentree-scolaire-ars-2019\n",
      "['Allocation de rentrée scolaire (ARS 2019)']\n",
      "[\"Qui a droit à l'allocation de rentrée scolaire 2019 - 2020 (ARS). Les conditions à remplir pour percevoir la prime de rentrée scolaire. Montant et date de versement de l'ARS en 2019. Les parents qui remplissent les conditions qui suivent...\"]\n",
      "================\n",
      "================\n",
      "http://www.lefigaro.fr/musique/2018/08/01/03006-20180801artfig00193-sanglante-bagarre-entre-les-rappeurs-booba-et-kaaris-a-l-aeroport-d-orly.php?redirect_premium\n",
      "['Page introuvable - Le Figaro']\n",
      "[]\n",
      "================\n",
      "================\n",
      "http://premium.lefigaro.fr/\n",
      "['Le Figaro Premium - Actualité']\n",
      "['Accédez à l’intégralité des articles du Figaro en illimité. Politique, économie, culture, international… Retrouvez les analyses, les investigations et les enquêtes\\nexclusives réservées aux abonnés pour tout savoir et tout comprendre. Profitez également du journal en version numérique dès 22h.']\n",
      "================\n",
      "================\n",
      "https://cuisine.journaldesfemmes.fr/recette-dessert\n",
      "['Recettes de desserts faciles et rapides']\n",
      "['Les meilleures recettes desserts, classiques ou originaux, à réaliser rapidement et facilement à la maison.']\n",
      "================\n",
      "================\n",
      "https://www.lachainemeteo.com/meteo-france/ville-6560/previsions-meteo-bordeaux-aujourdhui\n",
      "['Meteo Bordeaux (33000) - Gironde : Prévisions METEO GRATUITE à 15 jours - La Chaîne Météo']\n",
      "[\"Meteo Bordeaux (33000) - 15 jours de prévision météo gratuite. Retrouvez le temps pour aujourd'hui, demain, ce week-end. Suivez l'évolution du temps, des températures et du vent.\"]\n",
      "================\n",
      "================\n",
      "https://www.journaldesfemmes.fr/horoscope/chinois/chien-jour/\n",
      "['Horoscope Chien gratuit du jour']\n",
      "['Votre horoscope Chien gratuit du jour vous réserve bien des surprises : Votre vie conjugale sera animée']\n",
      "================\n",
      "================\n",
      "https://www.journaldesfemmes.fr/horoscope/chinois/chien-demain/\n",
      "['Horoscope Chien gratuit de demain']\n",
      "['Votre horoscope Chien gratuit de demain vous réserve bien des surprises : Votre vie conjugale sera très protégée par les astres']\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "filename = \"url_Scraping.csv\"\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "df = df[0:10]\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    try:\n",
    "        url_addr = f\"https://{str(df.iloc[i,0])}\"\n",
    "        page = requests.get(f\"{url_addr}\")\n",
    "        tree = html.fromstring(page.content)\n",
    "        page_url = page.url\n",
    "        title = tree.xpath('//html/head/title/text()')\n",
    "        description = tree.xpath('//html/head/meta[@name=\"description\"]/@content')\n",
    "        ## REMOVE COMMENTS TO SAVE INTO url_Scrapping.csv\n",
    "        \n",
    "        #with open(filename, 'a') as f:\n",
    "        #    f.write(f\"{page_url.replace(',',';')},{title.replace(',',';')},{description.replace(',',';')} \\n\")\n",
    "        #self.log(f\"scrap added to {filename}\")\n",
    "        print(\"================\")\n",
    "        print(page_url)\n",
    "        print(title)\n",
    "        print(description)\n",
    "        #print(f\"{page_url.replace(',',';')},{title.replace(',',';')},{description.replace(',',';')} \\n\")\n",
    "        print(\"================\")\n",
    "    except:\n",
    "        print(f\"Skipped url {i}, error scrapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing URL data and creating url_Preprocess.csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"url_Scraping.csv\", delimiter=\",\")\n",
    "df = df.dropna(subset=['title', 'description']).drop(columns=\"Unnamed: 3\")\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    # taking care of www., .com, http and https.. and tokenizing\n",
    "    if df.iloc[i,0][4]==\"s\":\n",
    "        df.iloc[i,0] = str(df.iloc[i,0][8:]).replace(\"www.\",\"\").replace(\".comment\",\". comment\").replace(\".madame\",\". madame\").replace(\".com\",\"\").replace(\".php\",\"\").replace(\".fr\",\"\").replace(\".html\",\"\").replace(\".\",\" \").replace(\"-\",\" \").replace(\"0\",\"\").replace(\"1\",\"\").replace(\"2\",\"\").replace(\"3\",\"\").replace(\"4\",\"\").replace(\"5\",\"\").replace(\"6\",\"\").replace(\"7\",\"\").replace(\"8\",\"\").replace(\"9\",\"\").strip().split(\"/\")\n",
    "        df.iloc[i,0] = str(\" \".join(df.iloc[i,0]).strip().replace(\"     \",\" \").replace(\"  \",\" \").lower().replace(\" artfig\",\"\")).split(\" \")\n",
    "    else:\n",
    "        df.iloc[i,0] = str(df.iloc[i,0][7:]).replace(\"www.\",\"\").replace(\".comment\",\". comment\").replace(\".madame\",\". madame\").replace(\".com\",\"\").replace(\".php\",\"\").replace(\".fr\",\"\").replace(\".html\",\"\").replace(\".\",\" \").replace(\"-\",\" \").replace(\"0\",\"\").replace(\"1\",\"\").replace(\"2\",\"\").replace(\"3\",\"\").replace(\"4\",\"\").replace(\"5\",\"\").replace(\"6\",\"\").replace(\"7\",\"\").replace(\"8\",\"\").replace(\"9\",\"\").strip().split(\"/\")\n",
    "        df.iloc[i,0] = str(\" \".join(df.iloc[i,0]).strip().replace(\"     \",\" \").replace(\"  \",\" \").lower().replace(\" artfig\",\"\")).split(\" \")\n",
    "\n",
    "df.to_csv(\"url_Preprocess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"url_Preprocess.csv\")\n",
    "unallowed_words = [\"lefigaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in unallowed_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_url = df['url_clean'].map(preprocess)\n",
    "processed_url\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_url)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_url]\n",
    "\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')\n",
    "\n",
    "NUM_TOPICS = 8\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.058*\"meteo\" + 0.040*\"fiche\" + 0.040*\"questionnaire\" + 0.033*\"ville\" + 0.033*\"weekend\" + 0.029*\"previsions\" + 0.026*\"aujourdhui\" + 0.024*\"deco\" + 0.017*\"coloriages\" + 0.017*\"hugolescargot\"\n",
      "Topic: 1 \n",
      "Words: 0.052*\"sante\" + 0.047*\"article\" + 0.032*\"conseils\" + 0.019*\"editorial\" + 0.018*\"cadremploi\" + 0.016*\"pour\" + 0.015*\"vous\" + 0.011*\"sont\" + 0.009*\"comment\" + 0.007*\"actualite\"\n",
      "Topic: 2 \n",
      "Words: 0.044*\"finances\" + 0.043*\"droit\" + 0.024*\"immobilier\" + 0.015*\"pour\" + 0.015*\"guide\" + 0.014*\"article\" + 0.013*\"meghan\" + 0.013*\"management\" + 0.013*\"markle\" + 0.013*\"prince\"\n",
      "Topic: 3 \n",
      "Words: 0.124*\"forum\" + 0.121*\"affich\" + 0.028*\"comment\" + 0.014*\"pour\" + 0.014*\"auto\" + 0.011*\"bricolage\" + 0.010*\"enfants\" + 0.010*\"plus\" + 0.009*\"faire\" + 0.008*\"avec\"\n",
      "Topic: 4 \n",
      "Words: 0.092*\"cuisine\" + 0.055*\"recette\" + 0.053*\"madame\" + 0.045*\"recettes\" + 0.015*\"gastronomie\" + 0.015*\"sport\" + 0.010*\"idees\" + 0.009*\"beaute\" + 0.008*\"foot\" + 0.007*\"pour\"\n",
      "Topic: 5 \n",
      "Words: 0.084*\"madame\" + 0.043*\"video\" + 0.040*\"celebrites\" + 0.021*\"photos\" + 0.019*\"figaro\" + 0.016*\"style\" + 0.016*\"societe\" + 0.011*\"mariage\" + 0.011*\"ticketac\" + 0.009*\"spectacles\"\n",
      "Topic: 6 \n",
      "Words: 0.050*\"etre\" + 0.048*\"bien\" + 0.046*\"madame\" + 0.022*\"leconjugueur\" + 0.021*\"horoscope\" + 0.019*\"conjugaison\" + 0.019*\"verbe\" + 0.018*\"code\" + 0.015*\"promo\" + 0.015*\"codespromo\"\n",
      "Topic: 7 \n",
      "Words: 0.073*\"sante\" + 0.058*\"article\" + 0.033*\"etudiant\" + 0.024*\"actualite\" + 0.021*\"medecine\" + 0.018*\"immobilier\" + 0.016*\"resultat\" + 0.014*\"societe\" + 0.013*\"plus\" + 0.011*\"symptomes\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.058*\"meteo\"')\n",
      "(1, '0.052*\"sante\"')\n",
      "(2, '0.044*\"finances\"')\n",
      "(3, '0.124*\"forum\"')\n",
      "(4, '0.092*\"cuisine\"')\n",
      "(5, '0.084*\"madame\"')\n",
      "(6, '0.050*\"etre\"')\n",
      "(7, '0.073*\"sante\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Analysing Titles with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"url_Preprocess.csv\")\n",
    "unallowed_words = [\"lefigaro\",\"figaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\",\"internaute\",\"madame\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 4 and token not in unallowed_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_url = df['title'].map(preprocess)\n",
    "processed_url\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_url)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_url]\n",
    "\n",
    "NUM_TOPICS = 9\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.071*\"meteo\" + 0.041*\"météo\" + 0.040*\"jours\" + 0.039*\"gratuite\" + 0.038*\"chaîne\" + 0.036*\"prévisions\" + 0.035*\"résolu\" + 0.009*\"espagne\" + 0.008*\"académie\" + 0.008*\"résultat\"\n",
      "Topic: 1 \n",
      "Words: 0.031*\"conseils\" + 0.021*\"lettre\" + 0.021*\"fiches\" + 0.019*\"médicaux\" + 0.018*\"santé\" + 0.016*\"modèle\" + 0.015*\"tirage\" + 0.014*\"résultat\" + 0.011*\"villes\" + 0.011*\"ligne\"\n",
      "Topic: 2 \n",
      "Words: 0.143*\"recette\" + 0.043*\"facile\" + 0.042*\"cuisine\" + 0.019*\"meilleure\" + 0.017*\"faire\" + 0.016*\"france\" + 0.009*\"chocolat\" + 0.008*\"maison\" + 0.008*\"euros\" + 0.008*\"tarte\"\n",
      "Topic: 3 \n",
      "Words: 0.111*\"résolu\" + 0.051*\"santé\" + 0.045*\"médecine\" + 0.039*\"comment\" + 0.016*\"pourquoi\" + 0.010*\"symptômes\" + 0.008*\"nouveau\" + 0.008*\"bonne\" + 0.008*\"résumé\" + 0.008*\"cette\"\n",
      "Topic: 4 \n",
      "Words: 0.020*\"meghan\" + 0.020*\"définition\" + 0.017*\"markle\" + 0.011*\"dictionnaire\" + 0.010*\"facile\" + 0.010*\"générale\" + 0.010*\"simple\" + 0.010*\"votre\" + 0.007*\"fêtes\" + 0.006*\"match\"\n",
      "Topic: 5 \n",
      "Words: 0.029*\"gratuit\" + 0.025*\"conjugaison\" + 0.024*\"verbe\" + 0.024*\"horoscope\" + 0.023*\"résolu\" + 0.022*\"coloriage\" + 0.019*\"ligne\" + 0.016*\"imprimer\" + 0.016*\"grand\" + 0.012*\"meilleur\"\n",
      "Topic: 6 \n",
      "Words: 0.045*\"etudiant\" + 0.016*\"idées\" + 0.011*\"emploi\" + 0.009*\"avant\" + 0.009*\"comment\" + 0.009*\"hallyday\" + 0.008*\"prénoms\" + 0.008*\"reconnaissez\" + 0.008*\"johnny\" + 0.007*\"classement\"\n",
      "Topic: 7 \n",
      "Words: 0.029*\"cuisine\" + 0.023*\"recettes\" + 0.017*\"promo\" + 0.015*\"cette\" + 0.015*\"quelle\" + 0.014*\"comment\" + 0.010*\"enfants\" + 0.009*\"carte\" + 0.009*\"faciles\" + 0.008*\"passez\"\n",
      "Topic: 8 \n",
      "Words: 0.013*\"france\" + 0.011*\"macron\" + 0.011*\"mariage\" + 0.010*\"prince\" + 0.010*\"contre\" + 0.009*\"immobilier\" + 0.008*\"paris\" + 0.008*\"culture\" + 0.007*\"télévision\" + 0.007*\"monde\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.029*\"résultat\"')\n",
      "(1, '0.022*\"résolu\"')\n",
      "(2, '0.019*\"promo\"')\n",
      "(3, '0.097*\"résolu\"')\n",
      "(4, '0.131*\"recette\"')\n",
      "(5, '0.019*\"cette\"')\n",
      "(6, '0.070*\"meteo\"')\n",
      "(7, '0.066*\"santé\"')\n",
      "(8, '0.027*\"résolu\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Analysing Descriptions with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"url_Preprocess.csv\")\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 4:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_url = df['description'].map(preprocess)\n",
    "processed_url\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_url)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_url]\n",
    "\n",
    "#pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')\n",
    "\n",
    "NUM_TOPICS = 9\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.057*\"réponse\" + 0.055*\"meilleure\" + 0.019*\"bonjour\" + 0.014*\"votre\" + 0.011*\"faire\" + 0.006*\"comme\" + 0.006*\"notre\" + 0.006*\"permet\" + 0.005*\"problème\" + 0.005*\"avoir\"\n",
      "Topic: 1 \n",
      "Words: 0.017*\"meteo\" + 0.016*\"altitude\" + 0.016*\"latitude\" + 0.016*\"longitude\" + 0.008*\"europe\" + 0.008*\"espagne\" + 0.005*\"située\" + 0.004*\"partie\" + 0.004*\"maladie\" + 0.004*\"symptômes\"\n",
      "Topic: 2 \n",
      "Words: 0.026*\"découvrez\" + 0.013*\"coloriage\" + 0.012*\"résultats\" + 0.010*\"retrouvez\" + 0.008*\"figaro\" + 0.007*\"classement\" + 0.007*\"millions\" + 0.007*\"sélection\" + 0.007*\"paris\" + 0.006*\"notre\"\n",
      "Topic: 3 \n",
      "Words: 0.026*\"recette\" + 0.024*\"verbe\" + 0.018*\"temps\" + 0.011*\"facile\" + 0.010*\"personnes\" + 0.010*\"ingrédients\" + 0.010*\"conjugaison\" + 0.009*\"sucre\" + 0.009*\"minutes\" + 0.009*\"avoir\"\n",
      "Topic: 4 \n",
      "Words: 0.009*\"après\" + 0.006*\"depuis\" + 0.005*\"images\" + 0.005*\"macron\" + 0.005*\"france\" + 0.005*\"vidéo\" + 0.005*\"cette\" + 0.005*\"monde\" + 0.004*\"lundi\" + 0.004*\"femme\"\n",
      "Topic: 5 \n",
      "Words: 0.008*\"voici\" + 0.006*\"année\" + 0.005*\"france\" + 0.005*\"cette\" + 0.005*\"conseils\" + 0.005*\"comment\" + 0.005*\"selon\" + 0.005*\"villes\" + 0.004*\"entre\" + 0.004*\"peuvent\"\n",
      "Topic: 6 \n",
      "Words: 0.028*\"défaut\" + 0.027*\"description\" + 0.024*\"votre\" + 0.009*\"française\" + 0.008*\"emploi\" + 0.008*\"définition\" + 0.007*\"france\" + 0.006*\"recherche\" + 0.006*\"langue\" + 0.006*\"semaine\"\n",
      "Topic: 7 \n",
      "Words: 0.010*\"santé\" + 0.007*\"toutes\" + 0.006*\"conseils\" + 0.006*\"cette\" + 0.005*\"actrice\" + 0.005*\"explications\" + 0.005*\"markle\" + 0.005*\"meghan\" + 0.004*\"harry\" + 0.004*\"années\"\n",
      "Topic: 8 \n",
      "Words: 0.014*\"voici\" + 0.010*\"votre\" + 0.008*\"faire\" + 0.006*\"travail\" + 0.006*\"cette\" + 0.006*\"hallyday\" + 0.005*\"règles\" + 0.005*\"lettre\" + 0.004*\"quelques\" + 0.004*\"johnny\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.057*\"réponse\"')\n",
      "(1, '0.017*\"meteo\"')\n",
      "(2, '0.026*\"découvrez\"')\n",
      "(3, '0.026*\"recette\"')\n",
      "(4, '0.009*\"après\"')\n",
      "(5, '0.008*\"voici\"')\n",
      "(6, '0.028*\"défaut\"')\n",
      "(7, '0.010*\"santé\"')\n",
      "(8, '0.014*\"voici\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving model: Combined URL/TITLES/DESCRIPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"url_Preprocess.csv\")\n",
    "unallowed_words = [\"lefigaro\",\"figaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\",\"internaute\",\"madame\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 4 and token not in unallowed_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_url = df[['description', 'title', 'url_clean']].apply(lambda x: ' '.join(x).replace('[','').replace(']','').replace(\"'\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\";\",\"\"), axis=1).map(preprocess)\n",
    "processed_url\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_url)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_url]\n",
    "\n",
    "#pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')\n",
    "\n",
    "NUM_TOPICS = 9\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.045*\"meteo\" + 0.020*\"france\" + 0.015*\"ville\" + 0.012*\"jours\" + 0.011*\"météo\" + 0.011*\"aujourdhui\" + 0.010*\"chaîne\" + 0.010*\"gratuite\" + 0.010*\"découvrez\" + 0.009*\"previsions\"\n",
      "Topic: 1 \n",
      "Words: 0.040*\"forum\" + 0.040*\"réponse\" + 0.039*\"meilleure\" + 0.038*\"affich\" + 0.033*\"résolu\" + 0.019*\"comment\" + 0.019*\"verbe\" + 0.013*\"bonjour\" + 0.013*\"conjugaison\" + 0.008*\"temps\"\n",
      "Topic: 2 \n",
      "Words: 0.012*\"celebrites\" + 0.011*\"video\" + 0.009*\"fiche\" + 0.009*\"défaut\" + 0.008*\"coloriage\" + 0.008*\"questionnaire\" + 0.008*\"description\" + 0.008*\"meghan\" + 0.008*\"photos\" + 0.007*\"horoscope\"\n",
      "Topic: 3 \n",
      "Words: 0.006*\"voyage\" + 0.005*\"idées\" + 0.005*\"voici\" + 0.004*\"guide\" + 0.004*\"couleur\" + 0.004*\"vacances\" + 0.004*\"calendrier\" + 0.004*\"video\" + 0.003*\"nouveau\" + 0.003*\"cette\"\n",
      "Topic: 4 \n",
      "Words: 0.028*\"conseils\" + 0.013*\"article\" + 0.010*\"comment\" + 0.009*\"cadremploi\" + 0.008*\"hallyday\" + 0.008*\"votre\" + 0.008*\"editorial\" + 0.007*\"johnny\" + 0.007*\"dictionnaire\" + 0.007*\"astuces\"\n",
      "Topic: 5 \n",
      "Words: 0.040*\"sante\" + 0.021*\"santé\" + 0.010*\"médecine\" + 0.010*\"medecine\" + 0.009*\"comment\" + 0.007*\"maladie\" + 0.006*\"article\" + 0.006*\"causes\" + 0.006*\"quels\" + 0.006*\"contre\"\n",
      "Topic: 6 \n",
      "Words: 0.058*\"recette\" + 0.043*\"cuisine\" + 0.024*\"recettes\" + 0.015*\"facile\" + 0.007*\"faire\" + 0.007*\"sucre\" + 0.006*\"chocolat\" + 0.006*\"ingrédients\" + 0.006*\"personnes\" + 0.006*\"pommes\"\n",
      "Topic: 7 \n",
      "Words: 0.019*\"droit\" + 0.015*\"finances\" + 0.013*\"votre\" + 0.011*\"prince\" + 0.011*\"lettre\" + 0.010*\"promo\" + 0.008*\"voici\" + 0.007*\"harry\" + 0.007*\"logement\" + 0.006*\"travail\"\n",
      "Topic: 8 \n",
      "Words: 0.018*\"article\" + 0.015*\"etudiant\" + 0.012*\"immobilier\" + 0.012*\"paris\" + 0.006*\"societe\" + 0.005*\"ticketac\" + 0.005*\"spectacles\" + 0.005*\"français\" + 0.005*\"euros\" + 0.004*\"grand\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.045*\"meteo\"')\n",
      "(1, '0.040*\"forum\"')\n",
      "(2, '0.012*\"celebrites\"')\n",
      "(3, '0.006*\"voyage\"')\n",
      "(4, '0.028*\"conseils\"')\n",
      "(5, '0.040*\"sante\"')\n",
      "(6, '0.058*\"recette\"')\n",
      "(7, '0.019*\"droit\"')\n",
      "(8, '0.018*\"article\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
